import pandas as pd
import numpy as np
import os
import download_data as dld
from sklearn.model_selection import train_test_split
# import re
# import string
# import joblib
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score, f1_score
# from scipy.spatial.distance import cosine
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# import nltk


# # T√©l√©charger les stopwords si n√©cessaire
# nltk.download("stopwords")
# nltk.download("punkt")

class Mlearn:
    def __init__(self, train_path="quora_data/train.csv", ):
        """Initialisation de la classe avec le chemin du fichier d'entra√Ænement."""
        self.train_path = train_path
        # self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
        self.model = None # RandomForestClassifier(n_estimators=100, random_state=42)
        self.data = None

    def load_data(self):
        """Charge et nettoie les donn√©es."""
        print("üîπ Chargement des donn√©es...")

        if not os.path.exists(self.train_path):
            # √âtape 1: T√©l√©charger les donn√©es
            dld.download_kaggle_data()

            # √âtape 2: D√©compresser le fichier quora-question-pairs.zip dans quora_data
            dld.unzip_quora_zip()
            
            # √âtape 3: Renommer le fichier test.csv en test2.csv
            dld.rename_test_file()

            # √âtape 4: D√©compresser les fichiers .zip (train.csv.zip, test.csv.zip, etc.) dans quora_data
            dld.unzip_files()

            # √âtape 5: Supprimer les fichiers ZIP et les fichiers d√©zipp√©s apr√®s extraction
            dld.delete_zip_files()
        
        train_data = pd.read_csv(self.train_path)
        train_data = train_data.dropna()  # Suppression des valeurs manquantes
        train_data = train_data.sample(frac=1).reset_index(drop=True)  # M√©lange des donn√©es

        self.data = train_data
    
    def split_data(self):

        print("üîπ D√©coupage des donn√©es...")

        X = self.data[['question1', 'question2']]
        y = self.data['is_duplicate']

        # D√©couper en Train (70%), Validation (15%), Test (15%)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

        self.X_train = X_train
        self.X_test = X_test
        self.X_val = X_val
        self.y_train = y_train
        self.y_test = y_test
        self.y_val = y_val


    # def rndforest(self):
    #     """Entra√Ænement du mod√®le RandomForestClassifier."""
    #     print("ÔøΩÔøΩ Entra√Ænement du mod√®le RandomForestClassifier...")

    # def clean_text(self, text):
    #     """Nettoyage du texte (suppression des caract√®res sp√©ciaux, mise en minuscules, etc.)."""
    #     text = text.lower()
    #     text = re.sub(f"[{string.punctuation}]", "", text)  # Suppression de la ponctuation
    #     words = word_tokenize(text)
    #     words = [word for word in words if word not in stopwords.words("english")]
    #     return " ".join(words)

    def extract_features(self, X):
        """Extraction de caract√©ristiques pour le machine learning."""
        print("üîπ Extraction des caract√©ristiques...")

        # Nettoyage des questions
        df["clean_q1"] = df["question1"].astype(str).apply(self.clean_text)
        df["clean_q2"] = df["question2"].astype(str).apply(self.clean_text)

        # Vectorisation TF-IDF
        questions = pd.concat([df["clean_q1"], df["clean_q2"]], axis=0).values
        self.vectorizer.fit(questions)

        q1_tfidf = self.vectorizer.transform(df["clean_q1"]).toarray()
        q2_tfidf = self.vectorizer.transform(df["clean_q2"]).toarray()

        # Calcul de la similarit√© cosinus entre TF-IDF des deux questions
        df["cosine_similarity"] = [1 - cosine(q1_tfidf[i], q2_tfidf[i]) for i in range(len(df))]

        # Diff√©rence de longueur
        df["length_diff"] = df["question1"].astype(str).apply(len) - df["question2"].astype(str).apply(len)

        return df[["cosine_similarity", "length_diff"]], df["is_duplicate"]

    # def train_model(self, X, y):
    #     """Entra√Æne le mod√®le de classification."""
    #     print("üîπ Entra√Ænement du mod√®le...")
    #     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    #     self.model.fit(X_train, y_train)
    #     y_pred = self.model.predict(X_test)
    #     accuracy = accuracy_score(y_test, y_pred)
    #     f1 = f1_score(y_test, y_pred)
    #     print(f"‚úÖ Mod√®le entra√Æn√© ! Pr√©cision : {accuracy:.4f} | F1-score : {f1:.4f}")

    # def predict(self, question1, question2):
    #     """Pr√©dit si deux questions sont des doublons."""
    #     clean_q1 = self.clean_text(question1)
    #     clean_q2 = self.clean_text(question2)

    #     # Transformation en TF-IDF
    #     q1_tfidf = self.vectorizer.transform([clean_q1]).toarray()[0]
    #     q2_tfidf = self.vectorizer.transform([clean_q2]).toarray()[0]

    #     cosine_similarity = 1 - cosine(q1_tfidf, q2_tfidf)
    #     length_diff = abs(len(question1) - len(question2))

    #     X_new = np.array([[cosine_similarity, length_diff]])
    #     prediction = self.model.predict(X_new)[0]
    #     return "Doublon" if prediction == 1 else "Non doublon"

    # def save_model(self, filename="quora_model.pkl"):
    #     """Sauvegarde le mod√®le entra√Æn√©."""
    #     joblib.dump((self.model, self.vectorizer), filename)
    #     print(f"üíæ Mod√®le sauvegard√© sous {filename}.")

    # def load_model(self, filename="quora_model.pkl"):
    #     """Charge un mod√®le sauvegard√©."""
    #     self.model, self.vectorizer = joblib.load(filename)
    #     print(f"üìÇ Mod√®le charg√© depuis {filename}.")

# üî• Utilisation de la classe
if __name__ == "__main__":
    detector = Mlearn()

    detector.load_data()

    detector.split_data()

    print(detector.X_train)
    
    # √âtape 1: Chargement et pr√©paration des donn√©es
    # data = detector.load_data()
    
    # # √âtape 2: Extraction des caract√©ristiques
    # X, y = detector.extract_features(data)

    # # √âtape 3: Entra√Ænement du mod√®le
    # detector.train_model(X, y)

    # # √âtape 4: Sauvegarde du mod√®le
    # detector.save_model()

    # # Exemple de pr√©diction
    # question1 = "What is the best way to learn machine learning?"
    # question2 = "How can I start learning ML?"
    # print(f"üí° Pr√©diction : {detector.predict(question1, question2)}")
